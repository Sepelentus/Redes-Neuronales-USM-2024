{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook veremos como implementar Mini Batch, tanto para el uso en redes densas como convolucionales, agregar a lo ultimo la funcion de MaxPooling y AveragePooling para reducir la dimensionalidad de cada una.\n",
    "\n",
    "Lo primero sera hacer el modelo de Network, con su funcion de activacion, de capas y otras metricas necesarias para el funcionamiento optimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0):\n",
    "        #np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg\n",
    "    # Forward propagation\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Compute the gradient with respect to weights and biases\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "\n",
    "        # Sum the output_error over the batch dimension to match bias shape\n",
    "        self.bias -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
    "\n",
    "        return input_error\n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation_function, activation_prime_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_prime_function = activation_prime_function\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        return self.activation_function(self.input)\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error * self.activation_prime_function(self.input)\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        result = input_data\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward_propagation(result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_real, y_hat):\n",
    "        return np.mean(np.power(y_real - y_hat, 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_prime(x):\n",
    "        sig = Network.sigmoid(x)  # Calculamos sigmoid(x) para cada elemento del vector\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_prime(x):\n",
    "        # Versión aproximada de ReLu', porque ReLu no es redivable en x=0\n",
    "        # La derivada de ReLU es 1 si x > 0, y 0 si x <= 0\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_prime(y_real, y_hat):\n",
    "        return 2*(y_hat-y_real)\n",
    "\n",
    "    # train the network with mini-batch gradient descent\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate, batch_size):\n",
    "        samples = len(x_train)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            indices = np.arange(samples)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train = x_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            for j in range(0, samples, batch_size):\n",
    "                end = j + batch_size\n",
    "                if end > samples:\n",
    "                    end = samples\n",
    "                x_batch = x_train[j:end]\n",
    "                y_batch = y_train[j:end]\n",
    "\n",
    "                # # Check the shape before forward propagation\n",
    "                # print(f\"x_batch shape: {x_batch.shape}\")  # This should be (32, 28, 28, 1) for ConvLayer\n",
    "\n",
    "                # Forward pass\n",
    "                output = x_batch  # Ensure x_batch is not flattened here\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # Compute loss (ensure it's scalar)\n",
    "                batch_err = np.mean(self.loss(y_batch, output))  # Reduce the error to a scalar\n",
    "                err += batch_err\n",
    "\n",
    "                # Backward pass\n",
    "                error = self.loss_prime(y_batch, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # Print average error per epoch\n",
    "            err /= (samples // batch_size)  # Average over samples\n",
    "            print(f'epoch {i + 1}/{epochs}   error={err/(samples // batch_size):.6f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de este codigo, se encuentra lo siguiente:\n",
    "    Layer: Viene siendo lo que representa la base de una neurona, que viene siendo el forward propagation y backward propagation, para modificacion de pesos y sesgos, entre otras funciones de una neurona.\n",
    "    FCLayer: Esto es lo mismo que lo anterior, pero para una red densa, donde se inicializa el peso y sesgo\n",
    "    ActivationLayer: Funcion de activacion, se le aplica a las salidas de la capa anterior.\n",
    "    Network: Donde se inicializa la red de neuronas, dentro de ella hay funciones de forward propagation y backward propagation.\n",
    "    Training (fit): Aqui es donde se genera el loop y el gradiente de mini batch es utilizado, donde itera en el dataset multiples veces (esto se representa en los epochs)\n",
    "    Loop de Fit: Dentro del loop se encuentran secciones apuntadas a minimizar el mini_batch de cada iteracion, con el objetivo de no entregar el dataset entero de una vez\n",
    "    Backward propagation: Funcion donde se calculan los pesos y sesgos\n",
    "    Forward propagation: Funcion que mueve la data a traves de la red\n",
    "    MSE: Funcion para saber la perdida de cada iteracion.\n",
    "\n",
    "Ya con el codigo hecho, podemos hacer pruebas con una red densa, para ver su funcionamiento y que tan preciso puede llegar a hacer.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.000014\n",
      "epoch 2/20   error=0.000007\n",
      "epoch 3/20   error=0.000005\n",
      "epoch 4/20   error=0.000005\n",
      "epoch 5/20   error=0.000004\n",
      "epoch 6/20   error=0.000004\n",
      "epoch 7/20   error=0.000003\n",
      "epoch 8/20   error=0.000003\n",
      "epoch 9/20   error=0.000003\n",
      "epoch 10/20   error=0.000003\n",
      "epoch 11/20   error=0.000002\n",
      "epoch 12/20   error=0.000002\n",
      "epoch 13/20   error=0.000002\n",
      "epoch 14/20   error=0.000002\n",
      "epoch 15/20   error=0.000002\n",
      "epoch 16/20   error=0.000002\n",
      "epoch 17/20   error=0.000002\n",
      "epoch 18/20   error=0.000002\n",
      "epoch 19/20   error=0.000001\n",
      "epoch 20/20   error=0.000001\n",
      "\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 972    0    1    0    0    2    2    1    2    0]\n",
      " [   0 1124    1    3    0    0    2    1    4    0]\n",
      " [   5    0  999    3    1    1    3    6   14    0]\n",
      " [   0    0    7  979    0    8    0    3   10    3]\n",
      " [   1    0    3    0  959    0    4    0    1   14]\n",
      " [   2    0    0    5    1  871    4    1    5    3]\n",
      " [   7    2    1    1    5    4  932    0    6    0]\n",
      " [   0    8   11    4    2    1    0  984    7   11]\n",
      " [   2    1    1    2    4    2    2    1  956    3]\n",
      " [   5    3    1    8    5    3    2    5    6  971]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.975\n",
      "\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN (entrenamiento)\n",
      "[[5879    1    1    1    2    3    9    0   21    6]\n",
      " [   1 6690   17    5    8    2    0    6   10    3]\n",
      " [   9    4 5881   11   10    2    2   11   23    5]\n",
      " [   5    3   19 5993    2   19    1   15   54   20]\n",
      " [   6    6    5    0 5789    1    8    1    5   21]\n",
      " [  12    2    8    5    9 5345    8    0   19   13]\n",
      " [  18    3    0    0    6    3 5874    0   14    0]\n",
      " [   3   16   20    5   22    1    1 6147   16   34]\n",
      " [   7    6    4    9    4    5    5    0 5802    9]\n",
      " [  12    3    1   10   13    8    2   14   28 5858]] \n",
      "\n",
      "La exactitud de ENTRENAMIENTO del modelo ANN es: 0.988\n"
     ]
    }
   ],
   "source": [
    "# Se importa las librerias utilizadas\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical # No recuerdo si se podia usar keras, pero asumo que debido a que el profesor la utiilizo, deberia poder utilizarse :)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Se carga el set de data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.array([x.flatten() for x in X_train], dtype='float64')\n",
    "X_test = np.array([x.flatten() for x in X_test], dtype='float64')    \n",
    "\n",
    "# Se normaliza la data, esto debido a que si no tira error.\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Se define el tamaño de entrada, en este caso 784 debido a que MNIST se compone de una dimension de 28x28, donde da 784 entradas.\n",
    "entrada_dim = 784\n",
    "\n",
    "# Se crea la instancia del Network\n",
    "model = Network()\n",
    "\n",
    "lambda_reg = 0.001\n",
    "\n",
    "# Se agregan capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 256, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(model.sigmoid, model.sigmoid_prime))\n",
    "model.add(FCLayer(256, 256, lambda_reg=lambda_reg))\n",
    "model.add(ActivationLayer(model.sigmoid, model.sigmoid_prime))\n",
    "model.add(FCLayer(256, 10, lambda_reg=lambda_reg)) \n",
    "model.add(ActivationLayer(model.sigmoid, model.sigmoid_prime))\n",
    "\n",
    "# Se define el tamaño del batch, donde 32 suele ser un numero generalmente utilizado debido a que es un buen punto medio entre efectividad y eficiencia.\n",
    "batch_size = 32\n",
    "\n",
    "# Se modifica el learning rate debido a que no se consiguio el aprendizaje deseado, bajandolo de 0.1 a 0.01\n",
    "# El learning rate afecta que tanto se mueven los pesos y sesgo, valores mas bajos permiten que no se sobreajuste.\n",
    "model.use(model.mse, model.mse_prime)\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.01, batch_size=batch_size)\n",
    "\n",
    "# Se predice en la seccion de testeo\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Se pasan las predicciones a una clase (No se porque exactamente pero supongo que se deben pasar a clase para sacar la precision del modelo)\n",
    "y_hat_labels = np.argmax(y_hat, axis=1) \n",
    "y_test_labels = np.argmax(y_test, axis=1) \n",
    "\n",
    "# Se hace la matriz de confusion y se mide la precision del modelo. \n",
    "matriz_conf = confusion_matrix(y_test_labels, y_hat_labels)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test_labels, y_hat_labels)))\n",
    "\n",
    "# Lo mismo pero en el set de entrenamiento.\n",
    "y_hat_train = model.predict(X_train)\n",
    "\n",
    "y_hat_train_labels = np.argmax(y_hat_train, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Compute confusion matrix for training set\n",
    "matriz_conf_train = confusion_matrix(y_train_labels, y_hat_train_labels)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN (entrenamiento)')\n",
    "print(matriz_conf_train, '\\n')\n",
    "print('La exactitud de ENTRENAMIENTO del modelo ANN es: {:.3f}'.format(accuracy_score(y_train_labels, y_hat_train_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos que para las redes densas funciona de manera correcta, podemos probar con redes convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace la clase de la capa de convolucion.\n",
    "class ConvLayer:\n",
    "    def __init__(self, input_shape, filter_size, num_filters):\n",
    "        self.input_shape = input_shape # Forma del input, como altura, ancho.\n",
    "        self.filter_size = filter_size # El tamaño de la capa de filtro, por ejemplo que cada filtro sea 3x3\n",
    "        self.num_filters = num_filters # Numero de filtros utilizados\n",
    "        # Pesos (filtros) y sesgo\n",
    "        self.filters = np.random.randn(num_filters, input_shape[2], filter_size, filter_size) / filter_size**2\n",
    "        self.biases = np.zeros((num_filters, 1))\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        # Revisa si se respeta la estructura planteada\n",
    "        if len(input_data.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D input, but got {input_data.shape}\")\n",
    "        batch_size, h, w, c = input_data.shape  # Se organiza la informacion de la estructura en el shape(batch_size, height, width, channels)\n",
    "        output_dim = h - self.filter_size + 1\n",
    "\n",
    "        # Inicializa la salida con 0s\n",
    "        self.output = np.zeros((batch_size, output_dim, output_dim, self.num_filters))\n",
    "\n",
    "        for f in range(self.num_filters):\n",
    "            for i in range(output_dim):\n",
    "                for j in range(output_dim):\n",
    "                    region = input_data[:, i:i+self.filter_size, j:j+self.filter_size, :]\n",
    "\n",
    "                    self.output[:, i, j, f] = np.sum(region * self.filters[f].transpose(1, 2, 0), axis=(1, 2, 3)) + self.biases[f]\n",
    "\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # Aqui se puede modificar para las necesidades de cada uno, por ahora solo pasaremos.\n",
    "        # Se configuran los gradientes para los filtros de peso, sesgo y se actualizan.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que funcione correctamente las capas convolucionales, se deben agregar lo que se denomina Pooling, que es la reduccion de dimensionalidad de cada capa de convolucion, esto con el objetivo de reducir los parametros y por consecuencia la capacidad computacional que se requiere para poder obtener un resultado.\n",
    "\n",
    "Tambien al eliminar parametros, ayudan a evitar el overfitting, que es cuando hay una gran cantidad de datos que se podrian evitar o eliminar.\n",
    "\n",
    "Existen dos tipos de pooling, MaxPooling y AveragePooling, la diferencia siendo la siguiente:\n",
    "    MaxPooling: Toma el valor maximo de una ventana de Pooling, por ejemplo si tenemos una entrada de 2x2 [1, 2, 3, 4] se tomara 4, esto con el objetivo de tomar el dato mas prominente.\n",
    "    AveragePooling: Toma el valor promedio de una ventana de Pooling, utilizando el ejemplo anterior, [1, 2, 3, 4], se suman los valores y se dividen por la cantidad de valores ((1+2+3+4) / 4) = 2.5, este es menos agresivo y puede llevar a mejores resultados si la data utilizada tiene valores extremos muy altos y distintos, lo que podria llevar a errores en la capa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la capa de pooling, que sirve para reducir la dimensionalidad y se retiene la informacion importante, utilizamos MaxPooling, que toma el valor mayor para la reduccion y AveragePooling, que toma el valor medio en cambio.\n",
    "class MaxPoolLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        batch_size, h, w, c = input_data.shape\n",
    "        output_dim = h // self.pool_size\n",
    "        self.output = np.zeros((batch_size, output_dim, output_dim, c))\n",
    "\n",
    "        for i in range(output_dim):\n",
    "            for j in range(output_dim):\n",
    "                region = input_data[:, i*self.pool_size:(i+1)*self.pool_size, j*self.pool_size:(j+1)*self.pool_size, :]\n",
    "                self.output[:, i, j, :] = np.max(region, axis=(1, 2))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # De igual manera, ahora mismo no utilizamos la propagacion pero se puede implementar luego.\n",
    "        pass\n",
    "\n",
    "class AveragePoolLayer:\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        batch_size, h, w, c = input_data.shape\n",
    "        output_dim = h // self.pool_size\n",
    "        self.output = np.zeros((batch_size, output_dim, output_dim, c))\n",
    "\n",
    "        for i in range(output_dim):\n",
    "            for j in range(output_dim):\n",
    "                region = input_data[:, i*self.pool_size:(i+1)*self.pool_size, j*self.pool_size:(j+1)*self.pool_size, :]\n",
    "                self.output[:, i, j, :] = np.mean(region, axis=(1, 2))  \n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien necesitamos una capa para aplanar los datos, debido a que las redes convolucionales utilizan dos dimensiones o mas en los datos y el metodo de fitting que utilizamos solo ocupa 1 dimension, debemos pasar las dimensiones utilizadas a una que pueda usar nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input_shape = input_data.shape\n",
    "        batch_size = input_data.shape[0]\n",
    "        # flattened_size = np.prod(self.input_shape[1:]) # Se calcula el tamaño una vez aplanado, esto es utilizado para debugear errores\n",
    "        # print(f\"Flattened output size: {flattened_size}\") # print utilizado para debugear errores.\n",
    "        return input_data.reshape(batch_size, -1)  # Aplana y se modifica para la data que le sirve a la capa densa.\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se comienza el proceso... Si no da error, dejalo andar\n",
      "epoch 1/20   error=0.000013\n",
      "epoch 2/20   error=0.000004\n",
      "epoch 3/20   error=0.000003\n",
      "epoch 4/20   error=0.000003\n",
      "epoch 5/20   error=0.000002\n",
      "epoch 6/20   error=0.000002\n",
      "epoch 7/20   error=0.000002\n",
      "epoch 8/20   error=0.000002\n",
      "epoch 9/20   error=0.000001\n",
      "epoch 10/20   error=0.000001\n",
      "epoch 11/20   error=0.000001\n",
      "epoch 12/20   error=0.000001\n",
      "epoch 13/20   error=0.000001\n",
      "epoch 14/20   error=0.000001\n",
      "epoch 15/20   error=0.000001\n",
      "epoch 16/20   error=0.000001\n",
      "epoch 17/20   error=0.000001\n",
      "epoch 18/20   error=0.000001\n",
      "epoch 19/20   error=0.000001\n",
      "epoch 20/20   error=0.000001\n",
      "\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 976    0    1    1    0    0    0    1    1    0]\n",
      " [   1 1130    2    2    0    0    0    0    0    0]\n",
      " [   3    1 1013    1    3    0    0    8    3    0]\n",
      " [   1    0    2  995    0    2    0    3    5    2]\n",
      " [   0    0    1    1  972    0    3    0    0    5]\n",
      " [   2    0    1   12    0  866    4    1    5    1]\n",
      " [   8    2    3    0    6    2  935    0    2    0]\n",
      " [   1    1    6    3    0    0    0 1011    3    3]\n",
      " [   4    0    4    1    2    1    1    1  956    4]\n",
      " [   5    2    2    5   12    0    0    8    4  971]] \n",
      "\n",
      "La exactitud del modelo es: 0.9825\n"
     ]
    }
   ],
   "source": [
    "# Solo se haran comentarios en los lugares relevantes, donde no se haya comentado anteriormente.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Esto esta hecho para MNIST especificamente, lo sabemos porque ocupamos nuevamente la dimension de 28x28\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Mucha de esta parte es para resolucion de errores que no se entienden, por lo que se soluciono a base de ChatGPT y Fé\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, 28*28)).reshape(-1, 28, 28, 1)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 28*28)).reshape(-1, 28, 28, 1)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Print utilizado para debugear un error que se encontro anteriormente, que no se hacia la forma de manera correcta.\n",
    "# print(f\"Input shape to ConvLayer: {X_train.shape}\")\n",
    "\n",
    "# Build the model\n",
    "model = Network()\n",
    "\n",
    "# Se agregan las capas convolucionales a la red.\n",
    "model.add(ConvLayer(input_shape=(28, 28, 1), filter_size=3, num_filters=16))\n",
    "model.add(MaxPoolLayer(pool_size=2, stride=2))  # Pooling\n",
    "model.add(ConvLayer(input_shape=(13, 13, 16), filter_size=3, num_filters=32)) \n",
    "model.add(MaxPoolLayer(pool_size=2, stride=2))  # Pooling\n",
    "\n",
    "# Se aplana la capa para el funcionamiento correcto anteriormente mencionado.\n",
    "model.add(FlattenLayer())\n",
    "\n",
    "# Capas conectadas totalmente, Ocupamos 800 como parametro para las capas\n",
    "model.add(FCLayer(800, 128)) \n",
    "model.add(ActivationLayer(Network.relu, Network.relu_prime))\n",
    "model.add(FCLayer(128, 64)) \n",
    "model.add(ActivationLayer(Network.relu, Network.relu_prime))\n",
    "model.add(FCLayer(64, 10)) \n",
    "model.add(ActivationLayer(Network.sigmoid, Network.sigmoid_prime))\n",
    "\n",
    "\n",
    "# Se puede reducir los epochs y el tamaño de los batch para que se demore menos, debido a que esta implementacion no ocupa GPU, se va a demorar cerca de 2 horas\n",
    "\n",
    "print(\"Se comienza el proceso... Si no da error, dejalo andar\")\n",
    "model.use(Network.mse, Network.mse_prime)\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.01, batch_size=32)\n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "y_hat_labels = np.argmax(y_hat, axis=1)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "matriz_conf = confusion_matrix(y_test_labels, y_hat_labels)\n",
    "\n",
    "print('\\nMATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf, '\\n')\n",
    "\n",
    "accuracy = accuracy_score(y_test_labels, y_hat_labels)\n",
    "print(f\"La exactitud del modelo es: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
